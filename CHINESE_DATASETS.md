# ä¸­æ–‡æ•°æ®é›†ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¨èçš„ä¸­æ–‡æ•°æ®é›†

æ ¹æ®Hugging Faceæœç´¢ï¼Œä»¥ä¸‹æ˜¯é€‚åˆGPTè®­ç»ƒçš„ä¼˜è´¨ä¸­æ–‡è¯­æ–™åº“ï¼š

### ğŸ¥‡ æ¨èæ•°æ®é›†ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

#### 1. ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆwikimedia/wikipediaï¼‰
- **è´¨é‡**ï¼šâ­â­â­â­â­ æœ€é«˜è´¨é‡
- **è§„æ¨¡**ï¼šæ•°ç™¾ä¸‡æ¡ç›®
- **ç‰¹ç‚¹**ï¼šçŸ¥è¯†ä¸°å¯Œã€è¯­è¨€è§„èŒƒã€è¦†ç›–å¹¿æ³›
- **ä½¿ç”¨**ï¼šè®­ç»ƒè„šæœ¬å·²é»˜è®¤ä¼˜å…ˆä½¿ç”¨

```python
dataset = load_dataset("wikimedia/wikipedia", "20231101.zh", split="train[:10%]")
```

#### 2. Finewebä¸­æ–‡æ•™è‚²è¯­æ–™ï¼ˆopencsg/Fineweb-Edu-Chinese-V2.1ï¼‰
- **è´¨é‡**ï¼šâ­â­â­â­â­ é«˜è´¨é‡æ•™è‚²å†…å®¹
- **è§„æ¨¡**ï¼š~958M tokens
- **ç‰¹ç‚¹**ï¼šæ•™è‚²å†…å®¹ã€è¯­è¨€è§„èŒƒã€é€‚åˆé€šç”¨è®­ç»ƒ
- **ä½¿ç”¨**ï¼šè®­ç»ƒè„šæœ¬çš„ç¬¬äºŒé€‰æ‹©

```python
dataset = load_dataset("opencsg/Fineweb-Edu-Chinese-V2.1", split="train[:5%]")
```

#### 3. Beautiful Chineseï¼ˆSeikaijyu/Beautiful-Chineseï¼‰
- **è´¨é‡**ï¼šâ­â­â­â­ æ–‡å­¦æ€§å¼º
- **è§„æ¨¡**ï¼š~810kæ¡
- **ç‰¹ç‚¹**ï¼šä¼˜ç¾çš„ä¸­æ–‡è¡¨è¾¾ã€é€‚åˆæ–‡å­¦é£æ ¼
- **ä½¿ç”¨**ï¼šè®­ç»ƒè„šæœ¬çš„ç¬¬ä¸‰é€‰æ‹©

```python
dataset = load_dataset("Seikaijyu/Beautiful-Chinese", split="train[:10%]")
```

#### 4. CulturalXä¸­æ–‡éƒ¨åˆ†ï¼ˆuonlp/CulturaXï¼‰
- **è´¨é‡**ï¼šâ­â­â­â­ å¤šæ ·åŒ–å†…å®¹
- **è§„æ¨¡**ï¼š~7.18B tokensï¼ˆè¶…å¤§ï¼‰
- **ç‰¹ç‚¹**ï¼šå¤šè¯­è¨€ã€å¤§è§„æ¨¡ã€å¤šæ ·åŒ–

```python
# ä»…ä½¿ç”¨ä¸­æ–‡éƒ¨åˆ†
dataset = load_dataset("uonlp/CulturaX", "zh", split="train[:1%]")
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ä¸€ï¼šè‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰

è®­ç»ƒè„šæœ¬å·²æ›´æ–°ï¼Œä½¿ç”¨`--use_chinese`ä¼šè‡ªåŠ¨æŒ‰ä¼˜å…ˆçº§å°è¯•ï¼š

```bash
# è‡ªåŠ¨å°è¯•æœ€ä½³ä¸­æ–‡æ•°æ®é›†
python3 train_single_gpu.py \
    --model_size small \
    --use_chinese \
    --epochs 5 \
    --batch_size 16 \
    --fp16
```

è„šæœ¬ä¼šæŒ‰é¡ºåºå°è¯•ï¼š
1. ä¸­æ–‡ç»´åŸºç™¾ç§‘ âœ…
2. Finewebä¸­æ–‡æ•™è‚² âœ…
3. Beautiful Chinese âœ…
4. æœ¬åœ°ç¤ºä¾‹æ–‡ä»¶ âœ…
5. è‹±æ–‡å¤‡é€‰ âœ…

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨æŒ‡å®šæ•°æ®é›†

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ç‰¹å®šæ•°æ®é›†ï¼Œå¯ä»¥ä¿®æ”¹è„šæœ¬æˆ–ç›´æ¥æŒ‡å®šï¼š

```bash
# ä½¿ç”¨æœ¬åœ°æ–‡æœ¬æ–‡ä»¶
python3 train_single_gpu.py \
    --model_size small \
    --text_file data/my_chinese_corpus.txt \
    --epochs 5 \
    --batch_size 16 \
    --fp16
```

### æ–¹å¼ä¸‰ï¼šä½¿ç”¨å¿«é€Ÿè„šæœ¬

```bash
# ä½¿ç”¨train_chinese.shï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰
./train_chinese.sh small 5
```

## ğŸ“¦ å‡†å¤‡è‡ªå·±çš„ä¸­æ–‡è¯­æ–™

### æ­¥éª¤1ï¼šæ”¶é›†æ–‡æœ¬

åˆ›å»º`data/my_corpus.txt`ï¼Œæ¯è¡Œä¸€æ®µæ–‡æœ¬ï¼š

```
äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ã€‚
...ï¼ˆæ›´å¤šæ–‡æœ¬ï¼‰
```

### æ­¥éª¤2ï¼šè®­ç»ƒ

```bash
python3 train_single_gpu.py \
    --model_size small \
    --text_file data/my_corpus.txt \
    --epochs 10 \
    --batch_size 16 \
    --fp16
```

## ğŸ¯ ä¸åŒæ•°æ®é›†çš„é€‚ç”¨åœºæ™¯

| æ•°æ®é›† | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|--------|---------|------|
| ä¸­æ–‡ç»´åŸºç™¾ç§‘ | é€šç”¨çŸ¥è¯†ã€ç™¾ç§‘é—®ç­” | å‡†ç¡®ã€æƒå¨ã€çŸ¥è¯†ä¸°å¯Œ |
| Finewebæ•™è‚² | æ•™è‚²å†…å®¹ã€å­¦ä¹ ææ–™ | è§„èŒƒã€ä¸“ä¸šã€æ˜“ç†è§£ |
| Beautiful Chinese | æ–‡å­¦åˆ›ä½œã€ä¼˜ç¾è¡¨è¾¾ | æ–‡é‡‡å¥½ã€è¡¨è¾¾ä¼˜ç¾ |
| è‡ªå®šä¹‰è¯­æ–™ | ç‰¹å®šé¢†åŸŸ | å¯æ§æ€§å¼ºã€é’ˆå¯¹æ€§å¼º |

## ğŸ’¡ æ•°æ®é›†å¤§å°å»ºè®®

æ ¹æ®æ‚¨çš„è®­ç»ƒç›®æ ‡ï¼š

### å¿«é€ŸéªŒè¯ï¼ˆ1-2å°æ—¶ï¼‰
```python
split="train[:1%]"  # ä½¿ç”¨1%æ•°æ®
```

### å¸¸è§„è®­ç»ƒï¼ˆ4-8å°æ—¶ï¼‰
```python
split="train[:10%]"  # ä½¿ç”¨10%æ•°æ®
```

### å®Œæ•´è®­ç»ƒï¼ˆ1-2å¤©ï¼‰
```python
split="train[:50%]"  # ä½¿ç”¨50%æ•°æ®
# æˆ–
split="train"  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
```

## ğŸ”§ æ•°æ®é›†é…ç½®ç¤ºä¾‹

### ä½¿ç”¨ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆ10%ï¼‰

```bash
python3 train_single_gpu.py \
    --model_size small \
    --use_chinese \
    --epochs 3 \
    --batch_size 16 \
    --fp16 \
    --output_dir ./output_wiki_zh \
    --model_save_dir ./gpt_model_wiki_zh
```

### ä½¿ç”¨Finewebæ•™è‚²è¯­æ–™ï¼ˆ5%ï¼‰

å·²åœ¨è„šæœ¬ä¸­é…ç½®ä¸ºå¤‡é€‰ï¼Œä¼šè‡ªåŠ¨å°è¯•ã€‚

### æ··åˆå¤šä¸ªæ•°æ®é›†

å¦‚æœè¦æ··åˆå¤šä¸ªæ•°æ®é›†ï¼Œéœ€è¦ä¿®æ”¹è„šæœ¬ï¼š

```python
from datasets import concatenate_datasets

# åŠ è½½å¤šä¸ªæ•°æ®é›†
wiki = load_dataset("wikimedia/wikipedia", "20231101.zh", split="train[:5%]")
edu = load_dataset("opencsg/Fineweb-Edu-Chinese-V2.1", split="train[:5%]")

# åˆå¹¶
dataset = concatenate_datasets([wiki, edu])
```

## ğŸŒ ç½‘ç»œé—®é¢˜è§£å†³

å¦‚æœæ•°æ®é›†ä¸‹è½½æ…¢æˆ–å¤±è´¥ï¼š

### 1. ä½¿ç”¨é•œåƒ
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

### 2. æ‰‹åŠ¨ä¸‹è½½
```bash
# ä½¿ç”¨huggingface-cliä¸‹è½½
pip install huggingface_hub
huggingface-cli download wikimedia/wikipedia --repo-type dataset
```

### 3. ä½¿ç”¨ä»£ç†
```bash
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
```

## ğŸ“Š æ•°æ®é›†è´¨é‡å¯¹æ¯”

| æ•°æ®é›† | è§„æ¨¡ | è´¨é‡ | ä¸‹è½½é€Ÿåº¦ | æ¨èåº¦ |
|--------|------|------|---------|--------|
| ä¸­æ–‡ç»´åŸº | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| Finewebæ•™è‚² | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ |
| Beautiful Chinese | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| æœ¬åœ°æ–‡ä»¶ | å¯æ§ | å¯æ§ | â­â­â­â­â­ | â­â­â­â­ |

## ğŸ¯ æ¨èç­–ç•¥

1. **é¦–é€‰**ï¼šè®©è„šæœ¬è‡ªåŠ¨é€‰æ‹©ï¼ˆå·²ä¼˜åŒ–ä¼˜å…ˆçº§ï¼‰
2. **ç½‘ç»œé—®é¢˜**ï¼šä½¿ç”¨æœ¬åœ°æ–‡æœ¬æ–‡ä»¶
3. **ç‰¹å®šé¢†åŸŸ**ï¼šå‡†å¤‡è‡ªå·±çš„è¯­æ–™

ç°åœ¨è®­ç»ƒè„šæœ¬å·²ç»é…ç½®å¥½è‡ªåŠ¨é€‰æ‹©æœ€ä½³ä¸­æ–‡æ•°æ®é›†ï¼
