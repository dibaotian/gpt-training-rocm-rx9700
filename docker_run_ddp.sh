#!/bin/bash
# Docker è·¨èŠ‚ç‚¹ DDP å¯åŠ¨è„šæœ¬

set -e

# æ£€æŸ¥å‚æ•°
if [ $# -lt 2 ]; then
    echo "=========================================="
    echo "Docker è·¨èŠ‚ç‚¹ DDP è®­ç»ƒå¯åŠ¨è„šæœ¬"
    echo "=========================================="
    echo ""
    echo "ç”¨æ³•: $0 <node_rank> <master_addr> [model_size] [epochs] [network_interface]"
    echo ""
    echo "å‚æ•°è¯´æ˜:"
    echo "  node_rank         : èŠ‚ç‚¹åºå· (0=ä¸»èŠ‚ç‚¹, 1=ä»èŠ‚ç‚¹)"
    echo "  master_addr       : ä¸»èŠ‚ç‚¹IPåœ°å€"
    echo "  model_size        : æ¨¡å‹å¤§å° (tiny/small/medium, é»˜è®¤:tiny)"
    echo "  epochs            : è®­ç»ƒè½®æ•° (é»˜è®¤:5)"
    echo "  network_interface : ç½‘ç»œæ¥å£å (é»˜è®¤:eth0)"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  ä¸»èŠ‚ç‚¹: $0 0 192.168.1.100"
    echo "  ä»èŠ‚ç‚¹: $0 1 192.168.1.100"
    echo ""
    echo "  è‡ªå®šä¹‰: $0 0 192.168.1.100 small 10 eno1"
    echo ""
    exit 1
fi

NODE_RANK=$1
MASTER_ADDR=$2
MODEL_SIZE=${3:-"tiny"}
EPOCHS=${4:-5}
NETWORK_INTERFACE=${5:-"eth0"}

# å›ºå®šé…ç½®
NNODES=2
NPROC_PER_NODE=1
MASTER_PORT=29500
WORLD_SIZE=$((NNODES * NPROC_PER_NODE))

# Docker é…ç½®
IMAGE_NAME="rocm/pytorch:rocm7.1_ubuntu22.04_py3.10_pytorch_release_2.8.0"
CONTAINER_NAME="gpt-train-node${NODE_RANK}"
SHM_SIZE="8G"

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# å¤„ç†DockeræŒ‚è½½å‘½åç©ºé—´é—®é¢˜ï¼š/dataåˆ†åŒºå¯èƒ½æ— æ³•è¢«Dockerè®¿é—®
if [[ "$SCRIPT_DIR" == /data/min/gpt_train ]]; then
    ORIGINAL_DIR="$SCRIPT_DIR"
    SCRIPT_DIR="${HOME}/Documents/min/gpt_train"
    echo "æ£€æµ‹åˆ°/dataè·¯å¾„ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºDockerå…¼å®¹è·¯å¾„"
    echo "  åŸè·¯å¾„: $ORIGINAL_DIR"
    echo "  æŒ‚è½½è·¯å¾„: $SCRIPT_DIR"
fi

echo "=========================================="
echo "Docker è·¨èŠ‚ç‚¹ DDP è®­ç»ƒå¯åŠ¨"
echo "=========================================="
echo ""
echo "èŠ‚ç‚¹é…ç½®:"
echo "  èŠ‚ç‚¹Rank: $NODE_RANK ($([ $NODE_RANK -eq 0 ] && echo 'ä¸»èŠ‚ç‚¹' || echo 'ä»èŠ‚ç‚¹'))"
echo "  æ€»èŠ‚ç‚¹æ•°: $NNODES"
echo "  ä¸»èŠ‚ç‚¹åœ°å€: $MASTER_ADDR"
echo "  ä¸»èŠ‚ç‚¹ç«¯å£: $MASTER_PORT"
echo "  World Size: $WORLD_SIZE"
echo ""
echo "è®­ç»ƒé…ç½®:"
echo "  æ¨¡å‹å¤§å°: $MODEL_SIZE"
echo "  è®­ç»ƒè½®æ•°: $EPOCHS"
echo "  ç½‘ç»œæ¥å£: $NETWORK_INTERFACE"
echo ""
echo "Docker é…ç½®:"
echo "  é•œåƒ: $IMAGE_NAME"
echo "  å®¹å™¨å: $CONTAINER_NAME"
echo "  å·¥ä½œç›®å½•: $SCRIPT_DIR"
echo "  ç½‘ç»œæ¨¡å¼: host"
echo ""

# æ£€æŸ¥Dockeræ˜¯å¦å®‰è£…
if ! command -v docker &> /dev/null; then
    echo "é”™è¯¯: Dockeræœªå®‰è£…"
    echo "è¯·è¿è¡Œ: sudo apt-get install -y docker.io"
    exit 1
fi

# æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨dockerç»„
if ! groups | grep -q docker; then
    echo "è­¦å‘Š: å½“å‰ç”¨æˆ·ä¸åœ¨dockerç»„ï¼Œä½¿ç”¨sudoè¿è¡Œ"
    DOCKER_CMD="sudo docker"
else
    DOCKER_CMD="docker"
fi

# æ£€æŸ¥GPUè®¾å¤‡
echo "æ£€æŸ¥GPUè®¾å¤‡..."
if [ ! -e /dev/kfd ] || [ ! -e /dev/dri ]; then
    echo "è­¦å‘Š: æœªæ‰¾åˆ°AMD GPUè®¾å¤‡"
    echo "  /dev/kfd: $([ -e /dev/kfd ] && echo 'âœ“' || echo 'âœ—')"
    echo "  /dev/dri: $([ -e /dev/dri ] && echo 'âœ“' || echo 'âœ—')"
else
    echo "  âœ“ GPUè®¾å¤‡æ­£å¸¸"
fi
echo ""

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§ï¼ˆä»èŠ‚ç‚¹æ£€æŸ¥ä¸»èŠ‚ç‚¹ï¼‰
if [ $NODE_RANK -ne 0 ]; then
    echo "æ£€æŸ¥ä¸ä¸»èŠ‚ç‚¹çš„ç½‘ç»œè¿é€šæ€§..."
    if ping -c 1 -W 2 $MASTER_ADDR &> /dev/null; then
        echo "  âœ“ å¯ä»¥pingé€šä¸»èŠ‚ç‚¹ $MASTER_ADDR"
    else
        echo "  âœ— æ— æ³•pingé€šä¸»èŠ‚ç‚¹ $MASTER_ADDR"
        echo "  è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        exit 1
    fi
    echo ""
fi

# æ‹‰å–é•œåƒï¼ˆå¦‚æœéœ€è¦ï¼‰
echo "æ£€æŸ¥Dockeré•œåƒ..."
if ! $DOCKER_CMD images | grep -q "rocm/pytorch.*rocm7.1"; then
    echo "æ­£åœ¨æ‹‰å–é•œåƒ (çº¦10GBï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)..."
    $DOCKER_CMD pull $IMAGE_NAME
else
    echo "  âœ“ é•œåƒå·²å­˜åœ¨"
fi
echo ""

# åœæ­¢å¹¶åˆ é™¤åŒåå®¹å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if $DOCKER_CMD ps -a | grep -q $CONTAINER_NAME; then
    echo "åœæ­¢ç°æœ‰å®¹å™¨ $CONTAINER_NAME..."
    $DOCKER_CMD stop $CONTAINER_NAME 2>/dev/null || true
    $DOCKER_CMD rm $CONTAINER_NAME 2>/dev/null || true
fi

# è®­ç»ƒå‚æ•°é…ç½®
case $MODEL_SIZE in
    tiny)
        BATCH_SIZE=16
        GRAD_ACCUM=8
        ;;
    small)
        BATCH_SIZE=8
        GRAD_ACCUM=16
        ;;
    medium)
        BATCH_SIZE=4
        GRAD_ACCUM=32
        ;;
    *)
        echo "é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹å¤§å° $MODEL_SIZE"
        echo "æ”¯æŒ: tiny, small, medium"
        exit 1
        ;;
esac

echo "å¯åŠ¨å®¹å™¨..."
echo "=========================================="
echo ""

# å¯åŠ¨å®¹å™¨
$DOCKER_CMD run -it --rm \
  --name $CONTAINER_NAME \
  --network host \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  --group-add render \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --ipc=host \
  --shm-size $SHM_SIZE \
  --mount type=bind,source="$SCRIPT_DIR",target=/workspace,bind-propagation=rslave \
  -w /workspace \
  -e MASTER_ADDR=$MASTER_ADDR \
  -e MASTER_PORT=$MASTER_PORT \
  -e RANK=$NODE_RANK \
  -e WORLD_SIZE=$WORLD_SIZE \
  -e NODE_RANK=$NODE_RANK \
  -e NCCL_SOCKET_IFNAME=$NETWORK_INTERFACE \
  -e NCCL_IB_DISABLE=1 \
  -e NCCL_DEBUG=INFO \
  -e NCCL_BUFFSIZE=2097152 \
  -e HSA_OVERRIDE_GFX_VERSION=12.0.1 \
  -e PYTORCH_ROCM_ARCH=gfx1201 \
  -e AMD_SERIALIZE_KERNEL=3 \
  -e GPU_MAX_HW_QUEUES=1 \
  -e HSA_ENABLE_SDMA=0 \
  -e HSA_FORCE_FINE_GRAIN_PCIE=1 \
  -e PYTORCH_HIP_ALLOC_CONF=max_split_size_mb:128 \
  -e HF_ENDPOINT=https://hf-mirror.com \
  $IMAGE_NAME \
  /bin/bash -c "
    echo '=========================================='
    echo 'Docker DDP å®¹å™¨å·²å¯åŠ¨'
    echo '=========================================='
    echo ''
    echo 'èŠ‚ç‚¹ä¿¡æ¯:'
    echo '  èŠ‚ç‚¹Rank: $NODE_RANK'
    echo '  ä¸»èŠ‚ç‚¹: $MASTER_ADDR:$MASTER_PORT'
    echo '  World Size: $WORLD_SIZE'
    echo ''
    echo 'ç¯å¢ƒé…ç½®:'
    echo '  NCCLæ¥å£: $NCCL_SOCKET_IFNAME'
    echo '  GPUæ¶æ„: $HSA_OVERRIDE_GFX_VERSION'
    echo ''
    echo 'ç¯å¢ƒä¿¡æ¯:'
    echo '  PyTorch: ' \$(python3 -c 'import torch; print(torch.__version__)' 2>/dev/null || echo 'æœªå®‰è£…')
    
    # æ£€æŸ¥GPU
    if python3 -c 'import torch; exit(0 if torch.cuda.is_available() else 1)' 2>/dev/null; then
        echo '  GPUå¯ç”¨: âœ“'
        echo '  GPUè®¾å¤‡: ' \$(python3 -c 'import torch; print(torch.cuda.get_device_name(0))')
        echo ''
        echo 'GPUçŠ¶æ€:'
        rocm-smi --showid --showproductname 2>/dev/null || rocm-smi 2>/dev/null || echo '  æ— æ³•è·å–GPUä¿¡æ¯'
    else
        echo '  GPUå¯ç”¨: âœ—'
        echo '  è­¦å‘Š: PyTorchæ— æ³•è®¿é—®GPU'
    fi
    echo ''
    
    # æ£€æŸ¥å¹¶è‡ªåŠ¨é€‰æ‹©ç½‘ç»œæ¥å£
    echo 'æ£€æŸ¥ç½‘ç»œæ¥å£...'
    if ip addr show $NETWORK_INTERFACE &> /dev/null; then
        echo \"  æŒ‡å®šæ¥å£: $NETWORK_INTERFACE\"
        ACTUAL_INTERFACE=$NETWORK_INTERFACE
    else
        echo \"  è­¦å‘Š: æ¥å£ $NETWORK_INTERFACE ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨é€‰æ‹©æ¥å£\"
        # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰IPçš„éloopbackæ¥å£
        ACTUAL_INTERFACE=\$(ip -br addr show | grep -v '^lo' | grep -v 'DOWN' | head -1 | awk '{print \$1}')
        if [ -n \"\$ACTUAL_INTERFACE\" ]; then
            echo \"  è‡ªåŠ¨é€‰æ‹©: \$ACTUAL_INTERFACE\"
        else
            echo \"  é”™è¯¯: æ‰¾ä¸åˆ°å¯ç”¨çš„ç½‘ç»œæ¥å£\"
            echo \"  å¯ç”¨æ¥å£åˆ—è¡¨:\"
            ip -br addr show
            exit 1
        fi
    fi
    
    # æ˜¾ç¤ºæ¥å£ä¿¡æ¯
    IP_ADDR=\$(ip addr show \$ACTUAL_INTERFACE | grep 'inet ' | awk '{print \$2}')
    echo \"  æ¥å£: \$ACTUAL_INTERFACE\"
    echo \"  IPåœ°å€: \$IP_ADDR\"
    echo ''
    
    # æ›´æ–°NCCLç¯å¢ƒå˜é‡
    export NCCL_SOCKET_IFNAME=\$ACTUAL_INTERFACE
    echo \"æ›´æ–° NCCL_SOCKET_IFNAME=\$ACTUAL_INTERFACE\"
    echo ''
    
    echo '=========================================='
    echo 'è®­ç»ƒé…ç½®:'
    echo '  æ¨¡å‹: $MODEL_SIZE'
    echo '  è½®æ•°: $EPOCHS'
    echo '  æ‰¹æ¬¡: $BATCH_SIZE'
    echo '  æ¢¯åº¦ç´¯ç§¯: $GRAD_ACCUM'
    echo '  æœ‰æ•ˆæ‰¹æ¬¡: \$((BATCH_SIZE * WORLD_SIZE * GRAD_ACCUM))'
    echo '=========================================='
    echo ''
    
    if [ $NODE_RANK -eq 0 ]; then
        echo 'ğŸš€ ä¸»èŠ‚ç‚¹å°±ç»ª'
        echo ''
        echo 'ç­‰å¾…ä»èŠ‚ç‚¹è¿æ¥...'
        echo 'è¯·åœ¨ä»èŠ‚ç‚¹ä¸Šè¿è¡Œ:'
        echo '  ./docker_run_ddp.sh 1 $MASTER_ADDR $MODEL_SIZE $EPOCHS $NETWORK_INTERFACE'
        echo ''
    else
        echo 'ğŸ“¡ ä»èŠ‚ç‚¹å°±ç»ª'
        echo ''
        echo 'æ­£åœ¨è¿æ¥ä¸»èŠ‚ç‚¹ $MASTER_ADDR:$MASTER_PORT ...'
        echo ''
    fi
    
    echo '----------------------------------------'
    echo 'é€‰æ‹©æ“ä½œ:'
    echo '  1. è‡ªåŠ¨å¼€å§‹è®­ç»ƒ (æ¨è)'
    echo '  2. è¿›å…¥äº¤äº’å¼shell (æ‰‹åŠ¨æ§åˆ¶)'
    echo '----------------------------------------'
    echo ''
    
    # ç­‰å¾…ç”¨æˆ·è¾“å…¥æˆ–è¶…æ—¶è‡ªåŠ¨å¼€å§‹
    read -t 10 -p 'è¯·é€‰æ‹© [1/2] (10ç§’åè‡ªåŠ¨é€‰æ‹©1): ' choice || choice=1
    echo ''
    
    if [ \"\$choice\" = \"2\" ]; then
        echo 'è¿›å…¥äº¤äº’å¼æ¨¡å¼...'
        echo ''
        echo 'æ‰‹åŠ¨å¯åŠ¨è®­ç»ƒå‘½ä»¤:'
        echo '  torchrun \\'
        echo '    --nproc_per_node=$NPROC_PER_NODE \\'
        echo '    --nnodes=$NNODES \\'
        echo '    --node_rank=$NODE_RANK \\'
        echo '    --master_addr=$MASTER_ADDR \\'
        echo '    --master_port=$MASTER_PORT \\'
        echo '    train_multi_gpu.py \\'
        echo '    --model_size $MODEL_SIZE \\'
        echo '    --use_chinese \\'
        echo '    --epochs $EPOCHS \\'
        echo '    --batch_size $BATCH_SIZE \\'
        echo '    --gradient_accumulation_steps $GRAD_ACCUM \\'
        echo '    --bf16'
        echo ''
        /bin/bash
    else
        echo '=========================================='
        echo 'ğŸš€ å¼€å§‹è®­ç»ƒ...'
        echo '=========================================='
        echo ''
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆä»…ä¸»èŠ‚ç‚¹ï¼‰
        if [ $NODE_RANK -eq 0 ]; then
            mkdir -p ./output_docker_ddp_${MODEL_SIZE}
            mkdir -p ./gpt_model_docker_ddp_${MODEL_SIZE}
        fi
        
        # å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        echo 'æ£€æŸ¥Pythonä¾èµ–...'
        if ! python3 -c 'import transformers' 2>/dev/null; then
            echo 'é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨å®‰è£…ä¾èµ–åŒ…...'
            pip3 install --no-cache-dir transformers datasets accelerate tensorboard tqdm -q
            echo 'âœ“ ä¾èµ–å®‰è£…å®Œæˆ'
        else
            echo 'âœ“ ä¾èµ–å·²å®‰è£…'
        fi
        echo ''
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿ä¸¤ä¸ªèŠ‚ç‚¹éƒ½å‡†å¤‡å¥½
        sleep 3
        
        # å¯åŠ¨è®­ç»ƒ
        torchrun \
            --nproc_per_node=$NPROC_PER_NODE \
            --nnodes=$NNODES \
            --node_rank=$NODE_RANK \
            --master_addr=$MASTER_ADDR \
            --master_port=$MASTER_PORT \
            train_multi_gpu.py \
            --model_size $MODEL_SIZE \
            --use_chinese \
            --epochs $EPOCHS \
            --batch_size $BATCH_SIZE \
            --gradient_accumulation_steps $GRAD_ACCUM \
            --max_length 512 \
            --bf16 \
            --output_dir ./output_docker_ddp_${MODEL_SIZE} \
            --model_save_dir ./gpt_model_docker_ddp_${MODEL_SIZE}
        
        echo ''
        echo '=========================================='
        echo 'âœ… è®­ç»ƒå®Œæˆï¼'
        echo '=========================================='
        
        if [ $NODE_RANK -eq 0 ]; then
            echo ''
            echo 'æ¨¡å‹ä¿å­˜: ./gpt_model_docker_ddp_${MODEL_SIZE}'
            echo 'æ—¥å¿—ä¿å­˜: ./output_docker_ddp_${MODEL_SIZE}/logs'
            echo ''
            echo 'æµ‹è¯•ç”Ÿæˆ:'
            echo '  python3 test_generation.py \\'
            echo '    --model_path ./gpt_model_docker_ddp_${MODEL_SIZE} \\'
            echo '    --prompt \"äººå·¥æ™ºèƒ½\" \\'
            echo '    --max_length 100'
            echo ''
        fi
        
        # è®­ç»ƒå®Œæˆåè¿›å…¥shell
        echo 'æŒ‰ä»»æ„é”®é€€å‡ºå®¹å™¨ï¼Œæˆ–ç»§ç»­ä½¿ç”¨shell...'
        /bin/bash
    fi
  "

echo ""
echo "å®¹å™¨å·²é€€å‡º"
echo ""
