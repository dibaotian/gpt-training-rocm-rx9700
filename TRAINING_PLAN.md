# GPTæ¨¡å‹è®­ç»ƒè®¡åˆ’ - RT9700 (ROCmç¯å¢ƒ)

## é¡¹ç›®æ¦‚è¿°

åœ¨AMD Radeon RX 9700ä¸Šè®­ç»ƒå°å‹GPTæ¨¡å‹ï¼Œåˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼š
- **é˜¶æ®µä¸€**ï¼šå•å¡è®­ç»ƒ - éªŒè¯ç¯å¢ƒã€ä»£ç å’ŒåŸºç¡€åŠŸèƒ½
- **é˜¶æ®µäºŒ**ï¼šå¤šæœºå¤šå¡è®­ç»ƒ - æ‰©å±•åˆ°åˆ†å¸ƒå¼è®­ç»ƒ

---

## é˜¶æ®µä¸€ï¼šå•å¡è®­ç»ƒ

### 1.1 ç¯å¢ƒå‡†å¤‡ï¼ˆDocker æ–¹å¼ï¼‰

#### ğŸ³ ä¸ºä»€ä¹ˆä½¿ç”¨ Dockerï¼Ÿ

- âœ… **é›¶é…ç½®**ï¼šé¢„è£… PyTorch 2.8.0 + ROCm 7.1ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®ç¯å¢ƒ
- âœ… **ç‰ˆæœ¬åŒ¹é…**ï¼šå®˜æ–¹ä¿è¯ PyTorch ä¸ ROCm ç‰ˆæœ¬å®Œå…¨å…¼å®¹
- âœ… **ç¯å¢ƒéš”ç¦»**ï¼šä¸å½±å“ä¸»æœºç³»ç»Ÿ
- âœ… **å¿«é€Ÿå¯åŠ¨**ï¼šå‡ åˆ†é’Ÿå³å¯å¼€å§‹è®­ç»ƒ

#### Docker ç¯å¢ƒå‡†å¤‡ï¼ˆ3æ­¥ï¼‰

**æ­¥éª¤ 1: å®‰è£… Docker**

```bash
# è¿è¡Œå®‰è£…è„šæœ¬
./install_docker.sh

# âš ï¸ é‡è¦ï¼šå®‰è£…å®Œæˆåå¿…é¡»é‡æ–°ç™»å½•ç³»ç»Ÿï¼
```

**æ­¥éª¤ 2: éªŒè¯ Docker å’Œ GPU**

```bash
# æ£€æŸ¥ Docker
docker --version

# æ£€æŸ¥ GPU è®¿é—®
docker run -it --rm \
  --device=/dev/kfd \
  --device=/dev/dri \
  rocm/rocm-terminal rocm-smi
```

**æ­¥éª¤ 3: å¯åŠ¨è®­ç»ƒå®¹å™¨**

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd gpt_train

# å¯åŠ¨ Docker å®¹å™¨ï¼ˆä½¿ç”¨æä¾›çš„è„šæœ¬ï¼‰
./docker_run.sh
```

#### å®¹å™¨å†…ç¯å¢ƒ

å®¹å™¨å·²é¢„è£…ï¼š
- âœ… PyTorch 2.8.0 (ROCm 7.1)
- âœ… Python 3.10
- âœ… ROCm å·¥å…·é“¾
- âœ… CUDA å…¼å®¹å±‚

éœ€è¦é¢å¤–å®‰è£…ï¼š
```bash
# åœ¨å®¹å™¨å†…æ‰§è¡Œï¼ˆé¦–æ¬¡å¯åŠ¨æ—¶ï¼‰
pip3 install -r requirements.txt

# requirements.txt åŒ…å«ï¼š
# transformers
# datasets
# tokenizers
# tensorboard
# wandb (å¯é€‰)
```

#### éªŒè¯ç¯å¢ƒ
```bash
# åœ¨å®¹å™¨å†…æ‰§è¡Œ
python3 -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
rocm-smi
```

### 1.2 æ¨¡å‹é€‰æ‹©

æ¨èä»å°åˆ°å¤§é€æ­¥å°è¯•ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒæ—¶é•¿(ä¼°ç®—) | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|---------------|----------|
| NanoGPT | 10M-100M | <2GB | å¿«é€Ÿ | å­¦ä¹ å’ŒéªŒè¯ |
| GPT-2 Small | 117M | ~2-3GB | ä¸­ç­‰ | ç”Ÿäº§ç¯å¢ƒå…¥é—¨ |
| DistilGPT-2 | 82M | ~2GB | å¿«é€Ÿ | è½»é‡çº§åº”ç”¨ |
| TinyLlama | 1.1B | ~10GB | è¾ƒé•¿ | æ›´å¼ºæ€§èƒ½ |

**æ¨èèµ·æ­¥**ï¼šNanoGPT (è‡ªå®šä¹‰é…ç½®ï¼Œ50Må‚æ•°)

### 1.3 æ•°æ®å‡†å¤‡

#### æ–¹æ¡ˆAï¼šä½¿ç”¨å…¬å¼€æ•°æ®é›†
```python
# ç¤ºä¾‹ï¼šä½¿ç”¨Hugging Face datasets
from datasets import load_dataset

# è‹±æ–‡æ•°æ®é›†
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

# ä¸­æ–‡æ•°æ®é›†
dataset = load_dataset("shibing624/chinese-c4-corpus", split="train[:1%]")
```

#### æ–¹æ¡ˆBï¼šè‡ªå®šä¹‰æ–‡æœ¬æ•°æ®
```bash
# å‡†å¤‡æ–‡æœ¬æ–‡ä»¶ (ä¾‹å¦‚ train.txt)
# æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªå¥å­æˆ–æ®µè½
mkdir -p data
# å°†æ‚¨çš„æ–‡æœ¬æ•°æ®æ”¾å…¥ data/train.txt
```

### 1.4 è®­ç»ƒè„šæœ¬

åˆ›å»ºåŸºç¡€è®­ç»ƒè„šæœ¬ `train_single_gpu.py`ï¼š

```python
import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# é…ç½®
model_name = "gpt2"  # æˆ–è‡ªå®šä¹‰é…ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½æ¨¡å‹å’Œtokenizer
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# å°å‹é…ç½®
config = GPT2Config(
    vocab_size=50257,
    n_positions=512,      # åºåˆ—é•¿åº¦
    n_embd=384,           # åµŒå…¥ç»´åº¦
    n_layer=6,            # å±‚æ•°
    n_head=6,             # æ³¨æ„åŠ›å¤´æ•°
)
model = GPT2LMHeadModel(config)
model.to(device)

# åŠ è½½æ•°æ®
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

# æ•°æ®é¢„å¤„ç†
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_steps=1000,
    save_total_limit=2,
    logging_steps=100,
    fp16=False,  # ROCmå¯èƒ½ä¸å®Œå…¨æ”¯æŒfp16ï¼Œå…ˆç”¨fp32
    evaluation_strategy="steps",
    eval_steps=500,
)

# è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜æ¨¡å‹
model.save_pretrained("./gpt_model")
tokenizer.save_pretrained("./gpt_model")
```

### 1.5 æ‰§è¡Œè®­ç»ƒï¼ˆDocker ç¯å¢ƒï¼‰

#### åŸºç¡€è®­ç»ƒ

```bash
# æ–¹æ³•1: ä½¿ç”¨æä¾›çš„è„šæœ¬ï¼ˆæ¨èï¼‰
./docker_run.sh

# å®¹å™¨å†…æ‰§è¡Œ
python3 train_single_gpu.py --model_size small

# ç›‘æ§GPUï¼ˆæ–°å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œåœ¨ä¸»æœºä¸Šæ‰§è¡Œï¼‰
watch -n 1 rocm-smi
```

#### ä¼˜åŒ–è®­ç»ƒï¼ˆæ¨èï¼‰

å¦‚æœæ‚¨é‡åˆ° GPU åˆ©ç”¨ç‡é«˜ä½† VRAM ä½¿ç”¨ç‡ä½çš„æƒ…å†µï¼Œä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼š

```bash
# åœ¨å®¹å™¨å†…æ‰§è¡Œ
./run_single_gpu_optimized.sh

# æˆ–ç›´æ¥è¿è¡Œä¼˜åŒ–è„šæœ¬
python3 train_single_gpu_optimized.py \
    --model_size small \
    --batch_size 32 \
    --gradient_accumulation_steps 4 \
    --fp16  # å¯é€‰ï¼šå¯ç”¨æ··åˆç²¾åº¦

# å‚è€ƒ GPU_TRAINING_OPTIMIZATION.md äº†è§£æ›´å¤šä¼˜åŒ–ç­–ç•¥
```

#### ç›‘æ§å’Œè°ƒè¯•

```bash
# ç»ˆç«¯1: è¿è¡Œè®­ç»ƒï¼ˆå®¹å™¨å†…ï¼‰
python3 train_single_gpu.py

# ç»ˆç«¯2: ç›‘æ§GPUï¼ˆä¸»æœºä¸Šï¼‰
watch -n 1 rocm-smi

# ç»ˆç«¯3: æŸ¥çœ‹å®¹å™¨æ—¥å¿—ï¼ˆä¸»æœºä¸Šï¼‰
docker logs -f gpt-train-container

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼ˆå®¹å™¨å†…ï¼‰
tensorboard --logdir=./output_single/logs
# ç„¶ååœ¨æµè§ˆå™¨è®¿é—®: http://localhost:6006
```

#### Docker å¸¸ç”¨æ“ä½œ

```bash
# æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨
docker ps

# è¿›å…¥å·²è¿è¡Œçš„å®¹å™¨
docker exec -it gpt-train-container bash

# åœæ­¢å®¹å™¨
docker stop gpt-train-container

# é‡å¯è®­ç»ƒ
./docker_run.sh
```

### 1.6 éªŒè¯å’Œæµ‹è¯•

```python
# æµ‹è¯•ç”Ÿæˆæ–‡æœ¬ test_generation.py
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("./gpt_model")
tokenizer = GPT2Tokenizer.from_pretrained("./gpt_model")

prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

---

## é˜¶æ®µäºŒï¼šå¤šæœºå¤šå¡è®­ç»ƒ

### 2.1 ç¯å¢ƒè¦æ±‚

#### ç¡¬ä»¶é…ç½®
- **å¤šä¸ªèŠ‚ç‚¹**ï¼šæ¯ä¸ªèŠ‚ç‚¹è‡³å°‘1å¼ AMD GPU
- **ç½‘ç»œ**ï¼šé«˜é€Ÿç½‘ç»œï¼ˆåƒå…†ä»¥ä¸Šï¼Œå»ºè®®ä¸‡å…†ï¼‰
- **å­˜å‚¨**ï¼šå…±äº«å­˜å‚¨ï¼ˆNFSï¼‰æˆ–ä¸€è‡´çš„æ•°æ®å‰¯æœ¬

#### è½¯ä»¶ä¾èµ–
```bash
# é™¤äº†é˜¶æ®µä¸€çš„ä¾èµ–ï¼Œè¿˜éœ€è¦ï¼š
# 1. RCCL (ROCm Communication Library)
# 2. MPI (OpenMPI æˆ– MPICH)
# 3. PyTorch Distributed

# å®‰è£…MPIï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev

# éªŒè¯RCCL
ls /opt/rocm/lib/librccl.so
```

### 2.2 ç½‘ç»œé…ç½®

#### NFSå…±äº«å­˜å‚¨ï¼ˆæ¨èï¼‰
```bash
# åœ¨ä¸»èŠ‚ç‚¹ä¸Šè®¾ç½®NFSæœåŠ¡å™¨
# å‚è€ƒæ‚¨çš„ nfs_setup.md æ–‡ä»¶

# ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹å¯ä»¥è®¿é—®ï¼š
# - è®­ç»ƒæ•°æ®
# - è®­ç»ƒè„šæœ¬
# - æ¨¡å‹ä¿å­˜è·¯å¾„
```

#### SSHå…å¯†ç™»å½•
```bash
# åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´é…ç½®SSHå…å¯†ç™»å½•
ssh-keygen -t rsa
ssh-copy-id user@node2
ssh-copy-id user@node3
```

### 2.3 åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥

#### æ•°æ®å¹¶è¡Œ (DDP - Distributed Data Parallel)
æœ€å¸¸ç”¨çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹å¼ï¼š
- æ¯ä¸ªGPUæŒæœ‰å®Œæ•´æ¨¡å‹å‰¯æœ¬
- æ•°æ®åˆ†ç‰‡åˆ°ä¸åŒGPU
- æ¢¯åº¦é€šè¿‡RCCLåŒæ­¥

#### è®­ç»ƒé…ç½®
```python
# train_multi_gpu.py
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.init_process_group(backend="nccl")  # RCCL backend
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    return local_rank

def main():
    local_rank = setup_distributed()
    
    # æ¨¡å‹é…ç½®
    config = GPT2Config(
        vocab_size=50257,
        n_positions=1024,
        n_embd=768,
        n_layer=12,
        n_head=12,
    )
    
    model = GPT2LMHeadModel(config)
    model.to(local_rank)
    
    # åŒ…è£…ä¸ºDDPæ¨¡å‹
    model = DDP(model, device_ids=[local_rank])
    
    # æ•°æ®åŠ è½½
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
    
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=1024)
    
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    
    # è®­ç»ƒå‚æ•°ï¼ˆå¤šGPUï¼‰
    training_args = TrainingArguments(
        output_dir="./output_distributed",
        overwrite_output_dir=True,
        num_train_epochs=5,
        per_device_train_batch_size=16,  # æ¯ä¸ªGPUçš„batch size
        per_device_eval_batch_size=16,
        gradient_accumulation_steps=4,   # æ¢¯åº¦ç´¯ç§¯
        save_steps=500,
        save_total_limit=3,
        logging_steps=50,
        evaluation_strategy="steps",
        eval_steps=250,
        local_rank=local_rank,
        ddp_backend="nccl",  # ä½¿ç”¨RCCL
        ddp_find_unused_parameters=False,
    )
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
    )
    
    trainer.train()
    
    # ä»…åœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹
    if local_rank == 0:
        model.module.save_pretrained("./gpt_model_distributed")
        tokenizer.save_pretrained("./gpt_model_distributed")

if __name__ == "__main__":
    main()
```

### 2.4 å¯åŠ¨è„šæœ¬

#### æ–¹æ¡ˆAï¼šä½¿ç”¨torchrunï¼ˆæ¨èï¼‰
```bash
#!/bin/bash
# run_distributed.sh

# å•æœºå¤šå¡
torchrun --nproc_per_node=4 \
         --nnodes=1 \
         --node_rank=0 \
         --master_addr="localhost" \
         --master_port=29500 \
         train_multi_gpu.py

# å¤šæœºå¤šå¡ - ä¸»èŠ‚ç‚¹
torchrun --nproc_per_node=4 \
         --nnodes=2 \
         --node_rank=0 \
         --master_addr="192.168.1.100" \
         --master_port=29500 \
         train_multi_gpu.py

# å¤šæœºå¤šå¡ - ä»èŠ‚ç‚¹ï¼ˆåœ¨node2ä¸Šæ‰§è¡Œï¼‰
torchrun --nproc_per_node=4 \
         --nnodes=2 \
         --node_rank=1 \
         --master_addr="192.168.1.100" \
         --master_port=29500 \
         train_multi_gpu.py
```

#### æ–¹æ¡ˆBï¼šä½¿ç”¨MPIå¯åŠ¨
```bash
#!/bin/bash
# run_mpi.sh

# åˆ›å»ºhostfile
cat > hostfile << EOF
node1 slots=4
node2 slots=4
EOF

# ä½¿ç”¨mpirunå¯åŠ¨
mpirun -np 8 \
       --hostfile hostfile \
       -x NCCL_DEBUG=INFO \
       -x NCCL_SOCKET_IFNAME=eth0 \
       python3 train_multi_gpu.py
```

### 2.5 ç¯å¢ƒå˜é‡é…ç½®

```bash
# å…³é”®ç¯å¢ƒå˜é‡
export NCCL_DEBUG=INFO              # è°ƒè¯•ä¿¡æ¯
export NCCL_SOCKET_IFNAME=eth0      # ç½‘ç»œæ¥å£
export NCCL_IB_DISABLE=1            # å¦‚æœæ²¡æœ‰InfiniBand
export GLOO_SOCKET_IFNAME=eth0      # Glooåç«¯
export MASTER_ADDR=192.168.1.100    # ä¸»èŠ‚ç‚¹IP
export MASTER_PORT=29500            # é€šä¿¡ç«¯å£
```

### 2.6 æ€§èƒ½ä¼˜åŒ–

#### æ¢¯åº¦ç´¯ç§¯
```python
# å‡å°‘é€šä¿¡é¢‘ç‡ï¼Œæé«˜ååé‡
training_args = TrainingArguments(
    gradient_accumulation_steps=8,  # ç´¯ç§¯8æ­¥å†æ›´æ–°
    ...
)
```

#### æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœæ”¯æŒï¼‰
```python
training_args = TrainingArguments(
    fp16=True,  # æˆ– bf16=True
    ...
)
```

#### ä¼˜åŒ–RCCLæ€§èƒ½
```bash
# ä½¿ç”¨TCPè€Œéå…±äº«å†…å­˜ï¼ˆå¤šèŠ‚ç‚¹ï¼‰
export NCCL_NET=Socket
export NCCL_SOCKET_IFNAME=eth0

# è°ƒæ•´RCCLç¼“å†²åŒº
export NCCL_BUFFSIZE=2097152
```

### 2.7 ç›‘æ§å’Œè°ƒè¯•

#### ç›‘æ§è®­ç»ƒè¿›åº¦
```bash
# ä½¿ç”¨tensorboard
tensorboard --logdir=./output_distributed/runs

# æˆ–ä½¿ç”¨wandb
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š
# import wandb
# wandb.init(project="gpt-training")
```

#### æ£€æŸ¥GPUåˆ©ç”¨ç‡
```bash
# åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šè¿è¡Œ
watch -n 1 rocm-smi

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python
```

#### å¸¸è§é—®é¢˜æ’æŸ¥
```bash
# æµ‹è¯•RCCLé€šä¿¡
cd rccl_install/rccl_multinode_test
./rccl_mpi_test

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping node2
ssh node2 "hostname"

# æŸ¥çœ‹RCCLæ—¥å¿—
export NCCL_DEBUG=INFO
# é‡æ–°è¿è¡Œè®­ç»ƒï¼ŒæŸ¥çœ‹è¯¦ç»†æ—¥å¿—
```

---

## é˜¶æ®µä¸‰ï¼šè¿›é˜¶ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### 3.1 æ¨¡å‹å¹¶è¡Œ

å¯¹äºæ›´å¤§çš„æ¨¡å‹ï¼ˆ>1Bå‚æ•°ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ¨¡å‹å¹¶è¡Œï¼š
```python
# ä½¿ç”¨DeepSpeedæˆ–Megatron-LM
# å°†æ¨¡å‹åˆ†ç‰‡åˆ°å¤šä¸ªGPU
```

### 3.2 FlashAttention

å¦‚æœROCmæ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨FlashAttentionåŠ é€Ÿï¼š
```python
from transformers import GPT2Config

config = GPT2Config(
    use_flash_attention=True,  # éœ€è¦ROCm 5.5+
)
```

### 3.3 æ£€æŸ¥ç‚¹å’Œæ¢å¤

```python
# è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
training_args = TrainingArguments(
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
)

# ä»æ£€æŸ¥ç‚¹æ¢å¤
trainer.train(resume_from_checkpoint="./output/checkpoint-1000")
```

---

## é™„å½•ï¼šå®Œæ•´å·¥ä½œæµç¨‹

### å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•ï¼ˆDocker ç¯å¢ƒï¼‰

#### é˜¶æ®µä¸€ï¼šå•å¡è®­ç»ƒï¼ˆDockerï¼‰
- [ ] å®‰è£… Docker
  ```bash
  ./install_docker.sh
  # é‡æ–°ç™»å½•ç³»ç»Ÿ
  ```
- [ ] éªŒè¯ Docker å’Œ GPU
  ```bash
  docker --version
  docker run -it --rm --device=/dev/kfd --device=/dev/dri rocm/rocm-terminal rocm-smi
  ```
- [ ] å¯åŠ¨è®­ç»ƒå®¹å™¨
  ```bash
  cd gpt_train
  ./docker_run.sh
  ```
- [ ] å®‰è£… Python ä¾èµ–ï¼ˆå®¹å™¨å†…ï¼Œé¦–æ¬¡ï¼‰
  ```bash
  pip3 install -r requirements.txt
  ```
- [ ] éªŒè¯ GPU å¯ç”¨æ€§ï¼ˆå®¹å™¨å†…ï¼‰
  ```bash
  python3 -c "import torch; print(torch.cuda.is_available())"
  rocm-smi
  ```
- [ ] è¿è¡Œè®­ç»ƒè„šæœ¬ï¼ˆå®¹å™¨å†…ï¼‰
  ```bash
  # åŸºç¡€è®­ç»ƒ
  python3 train_single_gpu.py --model_size tiny
  
  # æˆ–ä¼˜åŒ–è®­ç»ƒ
  ./run_single_gpu_optimized.sh
  ```
- [ ] ç›‘æ§ GPUï¼ˆä¸»æœºæ–°ç»ˆç«¯ï¼‰
  ```bash
  watch -n 1 rocm-smi
  ```
- [ ] éªŒè¯æ¨¡å‹ç”Ÿæˆï¼ˆå®¹å™¨å†…ï¼‰
  ```bash
  python3 test_generation.py
  ```

#### é˜¶æ®µäºŒï¼šå¤šæœºå¤šå¡è®­ç»ƒï¼ˆDockerï¼‰
- [ ] åœ¨æ‰€æœ‰èŠ‚ç‚¹å®‰è£… Docker
- [ ] é…ç½®å¤šèŠ‚ç‚¹ç½‘ç»œå’Œ SSH å…å¯†ç™»å½•
- [ ] è®¾ç½® NFS å…±äº«å­˜å‚¨ï¼ˆæ¨èï¼‰
  - æŒ‚è½½å…±äº«ç›®å½•åˆ°æ‰€æœ‰èŠ‚ç‚¹ç›¸åŒè·¯å¾„
- [ ] åœ¨æ¯ä¸ªèŠ‚ç‚¹æ‹‰å– Docker é•œåƒ
  ```bash
  docker pull rocm/pytorch:rocm7.1_ubuntu22.04_py3.10_pytorch_release_2.8.0
  ```
- [ ] æµ‹è¯•èŠ‚ç‚¹é—´ç½‘ç»œè¿é€šæ€§
  ```bash
  ping node2
  ssh node2 "docker run --rm rocm/rocm-terminal rocm-smi"
  ```
- [ ] å¯åŠ¨å¤šèŠ‚ç‚¹ Docker å®¹å™¨
  - ä¸»èŠ‚ç‚¹ï¼ˆnode1ï¼‰
    ```bash
    docker run -it --rm \
      --network=host \
      --device=/dev/kfd --device=/dev/dri \
      -v /shared/storage:/workspace \
      -e MASTER_ADDR=192.168.1.100 \
      -e MASTER_PORT=29500 \
      rocm/pytorch:rocm7.1_ubuntu22.04_py3.10_pytorch_release_2.8.0 \
      bash
    ```
  - ä»èŠ‚ç‚¹ï¼ˆnode2ï¼‰ï¼šç±»ä¼¼å‘½ä»¤
- [ ] å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå®¹å™¨å†…ï¼‰
  ```bash
  # ä¸»èŠ‚ç‚¹
  torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 \
    train_multi_gpu.py
  
  # ä»èŠ‚ç‚¹
  torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 \
    train_multi_gpu.py
  ```
- [ ] ç›‘æ§æ‰€æœ‰èŠ‚ç‚¹çš„è®­ç»ƒè¿›åº¦å’Œ GPU ä½¿ç”¨ç‡
  ```bash
  # åœ¨å„èŠ‚ç‚¹ä¸»æœºä¸Š
  watch -n 1 rocm-smi
  ```

---

## å‚è€ƒèµ„æº

### Docker ç›¸å…³
1. **ROCm Docker Hub**ï¼šhttps://hub.docker.com/r/rocm/pytorch
2. **ROCm Docker æ–‡æ¡£**ï¼šhttps://rocm.docs.amd.com/en/latest/deploy/docker.html
3. **DOCKER_SETUP.md**ï¼šé¡¹ç›®ä¸­çš„ Docker ç¯å¢ƒé…ç½®æŒ‡å—

### PyTorch å’Œè®­ç»ƒ
4. **PyTorch åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šhttps://pytorch.org/tutorials/intermediate/ddp_tutorial.html
5. **Hugging Face Trainer**ï¼šhttps://huggingface.co/docs/transformers/main_classes/trainer
6. **NanoGPT é¡¹ç›®**ï¼šhttps://github.com/karpathy/nanoGPT

### ROCm å’Œ RCCL
7. **RCCL æ–‡æ¡£**ï¼šhttps://github.com/ROCmSoftwarePlatform/rccl
8. **ROCm å…¼å®¹æ€§çŸ©é˜µ**ï¼šhttps://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html

### é¡¹ç›®æ–‡æ¡£
9. **GPU_TRAINING_OPTIMIZATION.md**ï¼šGPU è®­ç»ƒä¼˜åŒ–æŒ‡å—
10. **nfs_setup.md**ï¼šNFS å…±äº«å­˜å‚¨é…ç½®

---

## ä¸‹ä¸€æ­¥å»ºè®®

1. **ç«‹å³å¼€å§‹**ï¼šå…ˆå®Œæˆé˜¶æ®µä¸€çš„å•å¡è®­ç»ƒï¼ŒéªŒè¯ç¯å¢ƒ
2. **é€æ­¥æ‰©å±•**ï¼šå•å¡æˆåŠŸåï¼Œå†è¿›è¡Œå¤šæœºå¤šå¡
3. **æŒç»­ç›‘æ§**ï¼šä½¿ç”¨wandbæˆ–tensorboardè·Ÿè¸ªè®­ç»ƒ
4. **æ•°æ®ä¸ºç‹**ï¼šå‡†å¤‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®
5. **å°æ­¥å¿«è·‘**ï¼šä»å°æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ è§„æ¨¡
